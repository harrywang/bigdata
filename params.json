{
  "name": "Hadoop 101 Cookbook",
  "tagline": "A simple cookbook for installing and configuring Hadoop 2.6.0 on Ubuntu 14.04",
  "body": "This cookbook installs Hadoop 2.6.0 (single node cluster) on Ubuntu 14.04 and configures the system to run a simple word count python program (counting the words in the lyrics of the song \"[Imagine](https://www.youtube.com/watch?v=DVg2EJvvlF8)\" by John Lennon) using Hadoop Streaming API. Note that there are professionally developed cookbooks for Hadoop such as the one found at https://supermarket.chef.io/cookbooks/hadoop. This cookbook is for learning and teaching purpose and only tested on Mac.\r\n\r\nI referred to many online tutorials and articles as found in the references section at the end of this README - many thanks to those authors.\r\n\r\n### Instructions\r\nYou can follow the official tutorial at https://learn.chef.io/local-development/ubuntu/ to setup Chef local development environment or just follow the links in 1 and 2 below directly. For a brief introduction about the folder structure, see next section.\r\n\r\n1. Install Chef Development Kit at https://downloads.chef.io/chef-dk/mac/\r\n\r\n2. Install virtualization tools (VirtualBox and Vagrant) at https://learn.chef.io/local-development/rhel/get-set-up/\r\n\r\n3. run `kitchen converge` to start a Ubuntu instance and related configuration. Make sure you have fast Internet access when running this cookbook - we need to get many packages during this process, e.g., hadoop package itself is 186M. Other useful kitchen commands:\r\n    - `kitchen create`: In this step, Test Kitchen creates an instance of your virtual environment, for example, a CentOS 7 virtual machine.\r\n    - `kitchen converge`: In this step, Test Kitchen applies your cookbook to the virtual environment.\r\n    - `kitchen login`: In this step, Test Kitchen creates an SSH session into your virtual environment.\r\n    - `kitchen destroy`: In this step, Test Kitchen shuts down and destroys your virtual environment.\r\nIf things goes well, you have a Ubuntu 14.04 running with hadoop configured.\r\n\r\n4. login by running `kitchen login`\r\n\r\n    - `su hduser` enter 'test' as the password\r\n    - `cd ~` go to home\r\n    - `source ~/.bashrc` to setup environment (optional, if done, you can use `start-all.sh`, `hadoop`, `hdfs` commands without specifying the full path below)\r\n    - `/usr/local/hadoop/sbin/start-all.sh` to start hadoop use `jps` to check\r\n    - `/usr/local/hadoop/bin/hdfs dfs -mkdir -p /data/input` create hadoop input folder `/usr/local/hadoop/bin/hdfs dfs -rm -R /data/input` to remove\r\n    - `/usr/local/hadoop/bin/hdfs dfs -copyFromLocal imagine.txt /data/input` copy text file to input folder\r\n    - `/usr/local/hadoop/bin/hdfs dfs -ls /data/input` to view the input folder\r\n    - `/usr/local/hadoop/bin/hadoop jar hadoop-streaming-2.6.0.jar -file /home/hduser/wc_mapper.py -mapper /home/hduser/wc_mapper.py -file /home/hduser/wc_reducer.py -reducer /home/hduser/wc_reducer.py -input /data/input/* -output /data/output` to run the word count python mapper and reducer\r\n    - `/usr/local/hadoop/bin/hdfs dfs -ls /data/output` to view the output folder\r\n    - `/usr/local/hadoop/bin/hdfs dfs -cat /data/output/part-00000` to view the word count result\r\n    - `/usr/local/hadoop/bin/hdfs dfs -rm -R /data/output` remove the output folder first if you want to re-run the program.\r\n    - http://localhost:50070/ you can see the WebUI, if you need to do other part-forwarding, you can edit .kitchen.yml file.\r\n    - to shutdown the virtual Ubuntu, run `sudo poweroff`\r\n\r\n5. if you want to wipe out everything and start with a clean slate (in case something messed up), you can simply run `kitchen destroy` and then `kitchen converge` - Note: everything on the old virtual Ubuntu is deleted.\r\n\r\n### Cookbook Structure\r\nYou can use `tree` to generate the tree below.\r\n\r\n```\r\n.\r\n├── .kitchen.yml\r\n├── Berksfile\r\n├── Berksfile.lock\r\n├── LICENSE\r\n├── README.md\r\n├── attributes\r\n│   └── default.rb\r\n├── chefignore\r\n├── files\r\n│   ├── config\r\n│   │   ├── core-site.xml\r\n│   │   ├── hadoop-env.sh\r\n│   │   ├── hdfs-site.xml\r\n│   │   └── mapred-site.xml\r\n│   ├── data\r\n│   │   └── imagine.txt\r\n│   └── programs\r\n│       ├── wc_mapper.py\r\n│       └── wc_reducer.py\r\n├── metadata.rb\r\n├── recipes\r\n│   ├── clean.rb\r\n│   ├── default.rb\r\n│   ├── setup.rb\r\n│   └── word_count.rb\r\n├── templates\r\n│   └── default\r\n└── test\r\n    └── integration\r\n        └── default\r\n\r\n```\r\n\r\n- metadata.rb: basic information about the cookbook - the name is used in the run list\r\n- .kitchen.yml: specifies OS (ubuntu 14.04), port forwarding, and run-list (format: name_of_cookbook::name_of_recipe)\r\n- recipes: all configuration commands are stored in this folder\r\n- files: all files we need to copy to the instance are stored here\r\n- attributes: all attributes we need (I am not using any in this example)\r\n- templates: all templates files (.erb files)\r\n\r\n(Optional) Install Berkshelf: if you want to revise the cookbook to use external cookbooks, you need berkshelf: `gem install berkshelf`. If you are starting a new cookbook, you can use `berks cookbook your_cookbook_name` to initialize the folder structure (no need to do this for this cookbook - I have done it for you). Refer to the following tutorial is necessary: use external cookbook:\r\nhttp://docs.aws.amazon.com/opsworks/latest/userguide/cookbooks-101-opsworks-berkshelf.html#cookbooks-101-opsworks-berkshelf-vagrant\r\n\r\n- add external cookbook in Berksfile as `cookbook 'sudo', '~> 2.9.0'` you can go to https://supermarket.chef.io to search for a cookbook and the berkshelf information is there.\r\n- run `berks install` to download the external cookbook. If you are using Mac, the external cookbook is downloaded to ~/.berkshelf/cookbooks\r\n- specify the run list in .kitchen.yml\r\n- metadata.rb: this file specifies meta data for the cookbook, such as name, author, external cookbook dependencies, etc.\r\n- .kitchen.yml (use `kitchen init` to generate this file if not already exists): this files specifies information related to run the cookbook, such as driver (vagrant), os (ubuntu), which recipe(s) to run, and part forwarding.\r\n- Berksfile: this files specifies the source of external cookbooks, which is 'https://supermarket.chef.io'\r\n\r\n### Install Hadoop on Ubuntu 14.04 Command List\r\n\r\nIf you want to manually configure hadoop, you can copy and paste the following commands:\r\n\r\n```\r\nsudo apt-get --assume-yes update\r\nsudo apt-get --assume-yes install default-jdk\r\njava -version\r\nsudo addgroup hadoop\r\nsudo adduser --ingroup hadoop hduser\r\nsudo adduser hduser sudo\r\nsudo apt-get install ssh\r\nwhich ssh\r\nwhich sshd\r\nsu hduser\r\nssh-keygen -t rsa -P \"\"\r\ncat $HOME/.ssh/id_rsa.pub >> $HOME/.ssh/authorized_keys (authrized_keys is a file)\r\nssh localhost\r\ncd ~\r\nwget http://mirrors.sonic.net/apache/hadoop/common/hadoop-2.6.0/hadoop-2.6.0.tar.gz\r\ntar xvzf hadoop-2.6.0.tar.gz\r\ncd hadoop-2.6.0/\r\nsudo mkdir /usr/local/hadoop\r\nsudo mv * /usr/local/hadoop\r\nsudo chown -R hduser:hadoop /usr/local/hadoop\r\nupdate-alternatives --config java\r\n```\r\n\r\n`nano ~/.bashrc`, add the following to the end of the file (ctrl+o save, ctrl+x exit):\r\n\r\n```\r\n#HADOOP VARIABLES START\r\nexport JAVA_HOME=/usr/lib/jvm/java-7-openjdk-amd64\r\nexport HADOOP_INSTALL=/usr/local/hadoop\r\nexport PATH=$PATH:$HADOOP_INSTALL/bin\r\nexport PATH=$PATH:$HADOOP_INSTALL/sbin\r\nexport HADOOP_MAPRED_HOME=$HADOOP_INSTALL\r\nexport HADOOP_COMMON_HOME=$HADOOP_INSTALL\r\nexport HADOOP_HDFS_HOME=$HADOOP_INSTALL\r\nexport YARN_HOME=$HADOOP_INSTALL\r\nexport HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_INSTALL/lib/native\r\nexport HADOOP_OPTS=\"-Djava.library.path=$HADOOP_INSTALL/lib\"\r\n#HADOOP VARIABLES END\r\n```\r\n\r\n```\r\nsource ~/.bashrc\r\nwhich javac\r\nreadlink -f /usr/bin/javac /usr/lib/jvm/java-7-openjdk-amd64/bin/javac\r\n```\r\n`nano /usr/local/hadoop/etc/hadoop/hadoop-env.sh` revise: `export JAVA_HOME=/usr/lib/jvm/java-7-openjdk-amd64`\r\n\r\n```\r\nsudo mkdir -p /app/hadoop/tmp\r\nsudo chown hduser:hadoop /app/hadoop/tmp\r\n```\r\n\r\n`nano /usr/local/hadoop/etc/hadoop/core-site.xml`, enter the following (hadoop temp directory and hdfs uri):\r\n\r\n```\r\n<configuration>\r\n <property>\r\n  <name>hadoop.tmp.dir</name>\r\n  <value>/app/hadoop/tmp</value>\r\n  <description>A base for other temporary directories.</description>\r\n </property>\r\n\r\n <property>\r\n  <name>fs.default.name</name>\r\n  <value>hdfs://localhost:54310</value>\r\n  <description>The name of the default file system.  A URI whose\r\n  scheme and authority determine the FileSystem implementation.  The\r\n  uri's scheme determines the config property (fs.SCHEME.impl) naming\r\n  the FileSystem implementation class.  The uri's authority is used to\r\n  determine the host, port, etc. for a filesystem.</description>\r\n </property>\r\n</configuration>\r\n```\r\n\r\n```\r\ncp /usr/local/hadoop/etc/hadoop/mapred-site.xml.template /usr/local/hadoop/etc/hadoop/mapred-site.xml\r\nnano /usr/local/hadoop/etc/hadoop/mapred-site.xml\r\n```\r\nThe mapred-site.xml file is used to specify which framework is being used for MapReduce.\r\n\r\nenter the following:\r\n```\r\n<configuration>\r\n <property>\r\n  <name>mapred.job.tracker</name>\r\n  <value>localhost:54311</value>\r\n  <description>The host and port that the MapReduce job tracker runs\r\n  at.  If \"local\", then jobs are run in-process as a single map\r\n  and reduce task.\r\n  </description>\r\n </property>\r\n</configuration>\r\n```\r\n create two directories which will contain the namenode and the datanode for this Hadoop installation：\r\n```\r\n sudo mkdir -p /usr/local/hadoop_store/hdfs/namenode\r\n sudo mkdir -p /usr/local/hadoop_store/hdfs/datanode\r\n```\r\n `nano /usr/local/hadoop/etc/hadoop/hdfs-site.xml`, This file is used to specify the directories which will be used as the namenode and the datanode on that host, enter:\r\n\r\n```\r\n <configuration>\r\n <property>\r\n  <name>dfs.replication</name>\r\n  <value>1</value>\r\n  <description>Default block replication.\r\n  The actual number of replications can be specified when the file is created.\r\n  The default is used if replication is not specified in create time.\r\n  </description>\r\n </property>\r\n <property>\r\n   <name>dfs.namenode.name.dir</name>\r\n   <value>file:/usr/local/hadoop_store/hdfs/namenode</value>\r\n </property>\r\n <property>\r\n   <name>dfs.datanode.data.dir</name>\r\n   <value>file:/usr/local/hadoop_store/hdfs/datanode</value>\r\n </property>\r\n</configuration>\r\n```\r\n\r\nmake sure to use hduser: Format the New Hadoop Filesystem\r\n\r\n`hadoop namenode -format`\r\n\r\nNote that hadoop namenode -format command should be executed once before we start using Hadoop.\r\nIf this command is executed again after Hadoop has been used, it'll destroy all the data on the Hadoop file system.\r\n\r\nstart hadoop: you may need to go to /usr/local/hadoop/sbin to run the following commands:\r\n\r\n`start-all.sh` or (start-yarn.sh does not seem to start NameNode and DataNode). You may see the following messages:\r\n\r\n```\r\nThe authenticity of host 'localhost (::1)' can't be established.\r\nECDSA key fingerprint is 4c:94:0a:9e:a4:69:0f:f0:e8:c9:31:ac:0d:55:ba:36.\r\nAre you sure you want to continue connecting (yes/no)? yes\r\n\r\nThe authenticity of host '0.0.0.0 (0.0.0.0)' can't be established.\r\nECDSA key fingerprint is 4c:94:0a:9e:a4:69:0f:f0:e8:c9:31:ac:0d:55:ba:36.\r\nAre you sure you want to continue connecting (yes/no)? yes\r\n```\r\n\r\nuse jps (Java Virtual Machine Process Status Tool) to check whether hadoop is running or not：\r\n```\r\n$ jps\r\n14437 NameNode\r\n14559 DataNode\r\n14711 SecondaryNameNode\r\n14845 ResourceManager\r\n15226 Jps\r\n14942 NodeManager\r\n```\r\n`stop-all.sh` to stop hadoop\r\n\r\nhttp://localhost:50070/ is the web UI for NameNode daemon, you need to setup port forwarding on virtualbox for 50070 and 50090 (Settings --> Network --> part forwarding)\r\n\r\nhttp://localhost:50090/status.jsp check secondary namenode\r\n\r\nhttp://localhost:50090/logs/ to see logs\r\n\r\nYou can locally test the python mapper and reducer as follows:\r\n\r\n`echo \"foo foo bar labs foo bar\" | /home/hduser/wc_mapper.py`\r\n\r\n```\r\nfoo\t    1\r\nfoo\t    1\r\nbar\t    1\r\nlabs\t1\r\nfoo\t    1\r\nbar\t    1\r\n```\r\n\r\n`echo \"foo foo bar labs foo bar\" | /home/hduser/wc_mapper.py | sort -k1,1 | /home/hduser/wc_reducer.py`\r\n\r\n```\r\nbar\t    2\r\nfoo\t    3\r\nlabs\t1\r\n```\r\n\r\n`sort`: http://www.theunixschool.com/2012/08/linux-sort-command-examples.html\r\n\r\nexample: `sort -t\",\" -k1,1 file` The format of '-k' is : '-km,n' where m is the starting key and n is the ending key. In other words, sort can be used to sort on a range of fields just like how the group by in sql does. In our case, since the sorting is on the 1st field alone, we speciy '1,1'. Note: For a file which has fields delimited by a space or a tab, there is no need to specify the \"-t\" option since the white space is the delimiter by default in sort.\r\n\r\n### Other useful tips\r\n\r\n- To copy files from Ubuntu virtualbox: go to settings, add a shared folder, login to ubuntu, go to /media/your_shared_folder (you may need to add user `sudo adduser hduser vboxsf` and then reboot `sudo reboot`)\r\n\r\n### References\r\n\r\n- http://www.terpconnect.umd.edu/~kpzhang/ (special thanks to my friend Kunpeng for the course materials)\r\n- http://www.bogotobogo.com/Hadoop/BigData_hadoop_Install_on_ubuntu_single_node_cluster.php\r\n- http://www.michael-noll.com/tutorials/writing-an-hadoop-mapreduce-program-in-python/\r\n",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}