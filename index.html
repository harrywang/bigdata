<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="UTF-8">
    <title>Big Data 101 Cookbook by harrywang</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheets/normalize.css" media="screen">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-light.css" media="screen">
  </head>
  <body>
    <section class="page-header">
      <h1 class="project-name">Big Data 101 Cookbook</h1>
      <h2 class="project-tagline">A simple cookbook for installing and configuring a few systems for big data analytics</h2>
      <a href="https://github.com/harrywang/bigdata-cookbook" class="btn">View on GitHub</a>
      <a href="https://github.com/harrywang/bigdata-cookbook/zipball/master" class="btn">Download .zip</a>
      <a href="https://github.com/harrywang/bigdata-cookbook/tarball/master" class="btn">Download .tar.gz</a>
    </section>

    <section class="main-content">
      <p>This cookbook contains a number of recipes to setup a few systems for big data analytics. Note that there are professionally developed cookbooks for setting up those systems that can be found at <a href="https://supermarket.chef.io/">https://supermarket.chef.io/</a>. This cookbook is for learning and teaching purpose and only tested on Mac. I referred to many online tutorials and articles as found in the references section at the end of this README - many thanks to those authors.</p>

<ul>
<li>Hadoop: hadoop recipe installs Hadoop 2.6.0 (single node cluster) on Ubuntu 14.04 and configures the system to run a simple word count python program (counting the words in the lyrics of the song "<a href="https://www.youtube.com/watch?v=DVg2EJvvlF8">Imagine</a>" by John Lennon) using Hadoop Streaming API.</li>
<li>Spark: spark recipe install spark 1.6.0. pre-built for Hadoop 2.6 and later.</li>
</ul>

<h3>
<a id="instructions" class="anchor" href="#instructions" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Instructions</h3>

<p>You can follow the official tutorial at <a href="https://learn.chef.io/local-development/ubuntu/">https://learn.chef.io/local-development/ubuntu/</a> to setup Chef local development environment or just follow the links in 1 and 2 below directly. For a brief introduction about the folder structure, see next section.</p>

<ol>
<li><p>Install Chef Development Kit at <a href="https://downloads.chef.io/chef-dk/mac/">https://downloads.chef.io/chef-dk/mac/</a></p></li>
<li><p>Install virtualization tools (VirtualBox and Vagrant) at <a href="https://learn.chef.io/local-development/rhel/get-set-up/">https://learn.chef.io/local-development/rhel/get-set-up/</a></p></li>
<li><p>Use cookbook dependency manager Berkshelf to download external cookbooks: run <code>berks install</code> to download the external cookbook. If you are using Mac, the external cookbooks are downloaded to ~/.berkshelf/cookbooks. For example, I use java cookbook (<a href="https://supermarket.chef.io/cookbooks/java">https://supermarket.chef.io/cookbooks/java</a>) to install Oracle Java 7 - you can change the attributes in .kitchen.yml file (such as java version, etc).</p></li>
<li><p>Configuration (optional): the default system setting for this cookbook can be found in .kitchen.yml: Ubuntu 14.04, 2G RAM (512M is fine to run Hadoop example, more is need for Spark), some part forwarding settings. By default, this cookbook only installs hadoop with word count example. If you want to setup other systems, you need to uncomment the corresponding recipes in /recipes/default.rb.</p></li>
<li>
<p>run <code>kitchen converge</code> to start a Ubuntu instance and related configuration. Make sure you have fast Internet access when running this cookbook - we need to get many packages during this process, e.g., hadoop package itself is 186M. If things goes well, you have a Ubuntu 14.04 running with hadoop configured.</p>

<p>Other useful kitchen commands:</p>

<ul>
<li>
<code>kitchen create</code>: Test Kitchen creates an instance of your virtual environment, for example, a Ubuntu 14.04 virtual machine.</li>
<li>
<code>kitchen converge</code>: Test Kitchen applies your cookbook to the virtual environment, it also creates an instance if not already existed.</li>
<li>
<code>kitchen login</code>: Test Kitchen creates an SSH session into your virtual environment.</li>
<li>
<code>kitchen destroy</code>: Test Kitchen shuts down and destroys your virtual environment.</li>
</ul>
</li>
<li>
<p>login by running <code>kitchen login</code></p>

<p>You need to login as the dbuser:</p>

<ul>
<li>
<code>su bduser</code> enter 'test' as the password</li>
<li>
<code>cd ~</code> go to home</li>
<li>
<code>source ~/.bashrc</code> to setup environment (optional, if done, you can use <code>start-all.sh</code>, <code>hadoop</code>, <code>hdfs</code> commands without specifying the full path below)</li>
</ul>

<p>For Hadoop:</p>

<ul>
<li>
<code>/usr/local/hadoop/sbin/start-all.sh</code> to start hadoop use <code>jps</code> to check</li>
<li>
<code>/usr/local/hadoop/bin/hdfs dfs -mkdir -p /data/input</code> create hadoop input folder <code>/usr/local/hadoop/bin/hdfs dfs -rm -R /data/input</code> to remove</li>
<li>
<code>/usr/local/hadoop/bin/hdfs dfs -copyFromLocal ./data/imagine.txt /data/input</code> copy text file to input folder</li>
<li>
<code>/usr/local/hadoop/bin/hdfs dfs -ls /data/input</code> to view the input folder</li>
<li>
<code>/usr/local/hadoop/bin/hadoop jar hadoop-streaming-2.6.0.jar -mapper /home/bduser/programs/hadoop/wc_mapper.py -reducer /home/bduser/programs/hadoop/wc_reducer.py -input /data/input/* -output /data/output</code> to run the word count python mapper and reducer</li>
<li>
<code>/usr/local/hadoop/bin/hdfs dfs -ls /data/output</code> to view the output folder</li>
<li>
<code>/usr/local/hadoop/bin/hdfs dfs -cat /data/output/part-00000</code> to view the word count result</li>
<li>
<code>/usr/local/hadoop/bin/hdfs dfs -rm -R /data/output</code> remove the output folder first if you want to re-run the program.</li>
<li>http://localhost:50070/ you can see the WebUI, if you need to do other part-forwarding, you can edit .kitchen.yml file.</li>
<li>to shutdown the virtual Ubuntu, run <code>sudo poweroff</code>
</li>
</ul>

<p>For Spark:</p>

<ul>
<li><code>cd /home/bduser/spark-1.6.0-bin-hadoop2.6</code></li>
<li>To run Spark interactively in a Python interpreter: <code>./bin/pyspark --master local[2]</code>, the --master option specifies the master URL for a distributed cluster, or local to run locally with one thread, or local[N] to run locally with N threads.</li>
<li>Run an example: <code>./bin/spark-submit examples/src/main/python/pi.py 10</code>
</li>
</ul>
</li>
<li><p>if you want to wipe out everything and start with a clean slate (in case something messed up), you can simply run <code>kitchen destroy</code> and then <code>kitchen converge</code> - Note: everything on the old virtual Ubuntu is deleted.</p></li>
</ol>

<h3>
<a id="cookbook-structure" class="anchor" href="#cookbook-structure" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Cookbook Structure</h3>

<p>You can use <code>tree</code> to generate the tree below.</p>

<pre><code>.
├── .kitchen.yml
├── Berksfile
├── Berksfile.lock
├── LICENSE
├── README.md
├── attributes
├── chefignore
├── files
│   ├── config
│   ├── data
│   └── programs
├── metadata.rb
├── recipes
├── templates
└── test
</code></pre>

<ul>
<li>metadata.rb: this file specifies meta data for the cookbook, such as name, author, external cookbook dependencies, etc. The cookbook dependencies are also specified in this file.</li>
<li>Berksfile: this files specifies the source of external cookbooks (<a href="https://supermarket.chef.io">https://supermarket.chef.io</a>), and any external cookbooks</li>
<li>.kitchen.yml: specifies OS (ubuntu 14.04), port forwarding, and run-list (format: name_of_cookbook::name_of_recipe)</li>
<li>recipes: all configuration commands are stored in this folder</li>
<li>files: all files we need to copy to the instance are stored here</li>
<li>attributes: all attributes we need (I am not using any in this example)</li>
<li>templates: all templates files (.erb files)</li>
</ul>

<h3>
<a id="install-systems-on-ubuntu-1404-command-list" class="anchor" href="#install-systems-on-ubuntu-1404-command-list" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Install systems on Ubuntu 14.04 Command List</h3>

<h4>
<a id="hadoop" class="anchor" href="#hadoop" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Hadoop</h4>

<p>If you want to manually configure hadoop, you can copy and paste the following commands:</p>

<pre><code>sudo apt-get --assume-yes update
sudo apt-get --assume-yes install default-jdk
java -version
sudo addgroup hadoop
sudo adduser --ingroup hadoop bduser
sudo adduser bduser sudo
sudo apt-get install ssh
which ssh
which sshd
su bduser
ssh-keygen -t rsa -P ""
cat $HOME/.ssh/id_rsa.pub &gt;&gt; $HOME/.ssh/authorized_keys (authrized_keys is a file)
ssh localhost
cd ~
wget http://mirrors.sonic.net/apache/hadoop/common/hadoop-2.6.0/hadoop-2.6.0.tar.gz
tar xvzf hadoop-2.6.0.tar.gz
cd hadoop-2.6.0/
sudo mkdir /usr/local/hadoop
sudo mv * /usr/local/hadoop
sudo chown -R bduser:hadoop /usr/local/hadoop
update-alternatives --config java
</code></pre>

<p><code>nano ~/.bashrc</code>, add the following to the end of the file (ctrl+o save, ctrl+x exit):</p>

<pre><code>#HADOOP VARIABLES START
export JAVA_HOME=/usr/lib/jvm/java-7-openjdk-amd64
export HADOOP_INSTALL=/usr/local/hadoop
export PATH=$PATH:$HADOOP_INSTALL/bin
export PATH=$PATH:$HADOOP_INSTALL/sbin
export HADOOP_MAPRED_HOME=$HADOOP_INSTALL
export HADOOP_COMMON_HOME=$HADOOP_INSTALL
export HADOOP_HDFS_HOME=$HADOOP_INSTALL
export YARN_HOME=$HADOOP_INSTALL
export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_INSTALL/lib/native
export HADOOP_OPTS="-Djava.library.path=$HADOOP_INSTALL/lib"
#HADOOP VARIABLES END
</code></pre>

<pre><code>source ~/.bashrc
which javac
readlink -f /usr/bin/javac /usr/lib/jvm/java-7-openjdk-amd64/bin/javac
</code></pre>

<p><code>nano /usr/local/hadoop/etc/hadoop/hadoop-env.sh</code> revise: <code>export JAVA_HOME=/usr/lib/jvm/java-7-openjdk-amd64</code></p>

<pre><code>sudo mkdir -p /app/hadoop/tmp
sudo chown bduser:hadoop /app/hadoop/tmp
</code></pre>

<p><code>nano /usr/local/hadoop/etc/hadoop/core-site.xml</code>, enter the following (hadoop temp directory and hdfs uri):</p>

<pre><code>&lt;configuration&gt;
 &lt;property&gt;
  &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;
  &lt;value&gt;/app/hadoop/tmp&lt;/value&gt;
  &lt;description&gt;A base for other temporary directories.&lt;/description&gt;
 &lt;/property&gt;

 &lt;property&gt;
  &lt;name&gt;fs.default.name&lt;/name&gt;
  &lt;value&gt;hdfs://localhost:54310&lt;/value&gt;
  &lt;description&gt;The name of the default file system.  A URI whose
  scheme and authority determine the FileSystem implementation.  The
  uri's scheme determines the config property (fs.SCHEME.impl) naming
  the FileSystem implementation class.  The uri's authority is used to
  determine the host, port, etc. for a filesystem.&lt;/description&gt;
 &lt;/property&gt;
&lt;/configuration&gt;
</code></pre>

<pre><code>cp /usr/local/hadoop/etc/hadoop/mapred-site.xml.template /usr/local/hadoop/etc/hadoop/mapred-site.xml
nano /usr/local/hadoop/etc/hadoop/mapred-site.xml
</code></pre>

<p>The mapred-site.xml file is used to specify which framework is being used for MapReduce.</p>

<p>enter the following:</p>

<pre><code>&lt;configuration&gt;
 &lt;property&gt;
  &lt;name&gt;mapred.job.tracker&lt;/name&gt;
  &lt;value&gt;localhost:54311&lt;/value&gt;
  &lt;description&gt;The host and port that the MapReduce job tracker runs
  at.  If "local", then jobs are run in-process as a single map
  and reduce task.
  &lt;/description&gt;
 &lt;/property&gt;
&lt;/configuration&gt;
</code></pre>

<p>create two directories which will contain the namenode and the datanode for this Hadoop installation：</p>

<pre><code> sudo mkdir -p /usr/local/hadoop_store/hdfs/namenode
 sudo mkdir -p /usr/local/hadoop_store/hdfs/datanode
</code></pre>

<p><code>nano /usr/local/hadoop/etc/hadoop/hdfs-site.xml</code>, This file is used to specify the directories which will be used as the namenode and the datanode on that host, enter:</p>

<pre><code> &lt;configuration&gt;
 &lt;property&gt;
  &lt;name&gt;dfs.replication&lt;/name&gt;
  &lt;value&gt;1&lt;/value&gt;
  &lt;description&gt;Default block replication.
  The actual number of replications can be specified when the file is created.
  The default is used if replication is not specified in create time.
  &lt;/description&gt;
 &lt;/property&gt;
 &lt;property&gt;
   &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;
   &lt;value&gt;file:/usr/local/hadoop_store/hdfs/namenode&lt;/value&gt;
 &lt;/property&gt;
 &lt;property&gt;
   &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;
   &lt;value&gt;file:/usr/local/hadoop_store/hdfs/datanode&lt;/value&gt;
 &lt;/property&gt;
&lt;/configuration&gt;
</code></pre>

<p>make sure to use bduser: Format the New Hadoop Filesystem</p>

<p><code>hadoop namenode -format</code></p>

<p>Note that hadoop namenode -format command should be executed once before we start using Hadoop.
If this command is executed again after Hadoop has been used, it'll destroy all the data on the Hadoop file system.</p>

<p>start hadoop: you may need to go to /usr/local/hadoop/sbin to run the following commands:</p>

<p><code>start-all.sh</code> or (start-yarn.sh does not seem to start NameNode and DataNode). You may see the following messages:</p>

<pre><code>The authenticity of host 'localhost (::1)' can't be established.
ECDSA key fingerprint is 4c:94:0a:9e:a4:69:0f:f0:e8:c9:31:ac:0d:55:ba:36.
Are you sure you want to continue connecting (yes/no)? yes

The authenticity of host '0.0.0.0 (0.0.0.0)' can't be established.
ECDSA key fingerprint is 4c:94:0a:9e:a4:69:0f:f0:e8:c9:31:ac:0d:55:ba:36.
Are you sure you want to continue connecting (yes/no)? yes
</code></pre>

<p>use jps (Java Virtual Machine Process Status Tool) to check whether hadoop is running or not：</p>

<pre><code>$ jps
14437 NameNode
14559 DataNode
14711 SecondaryNameNode
14845 ResourceManager
15226 Jps
14942 NodeManager
</code></pre>

<p><code>stop-all.sh</code> to stop hadoop</p>

<p>http://localhost:50070/ is the web UI for NameNode daemon, you need to setup port forwarding on virtualbox for 50070 and 50090 (Settings --&gt; Network --&gt; part forwarding)</p>

<p>http://localhost:50090/status.jsp check secondary namenode</p>

<p>http://localhost:50090/logs/ to see logs</p>

<p>You can locally test the python mapper and reducer as follows:</p>

<p><code>echo "foo foo bar labs foo bar" | /home/bduser/wc_mapper.py</code></p>

<pre><code>foo     1
foo     1
bar     1
labs    1
foo     1
bar     1
</code></pre>

<p><code>echo "foo foo bar labs foo bar" | /home/bduser/wc_mapper.py | sort -k1,1 | /home/bduser/wc_reducer.py</code></p>

<pre><code>bar     2
foo     3
labs    1
</code></pre>

<p><code>sort</code>: <a href="http://www.theunixschool.com/2012/08/linux-sort-command-examples.html">http://www.theunixschool.com/2012/08/linux-sort-command-examples.html</a></p>

<p>example: <code>sort -t"," -k1,1 file</code> The format of '-k' is : '-km,n' where m is the starting key and n is the ending key. In other words, sort can be used to sort on a range of fields just like how the group by in sql does. In our case, since the sorting is on the 1st field alone, we specify '1,1'. Note: For a file which has fields delimited by a space or a tab, there is no need to specify the "-t" option since the white space is the delimiter by default in sort.</p>

<h4>
<a id="spark" class="anchor" href="#spark" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Spark</h4>

<p>If you want to manually configure spark, you can copy and paste the following commands (make sure hadoop 2.6.0 has been installed and configured):</p>

<h3>
<a id="other-useful-tips" class="anchor" href="#other-useful-tips" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Other useful tips</h3>

<ul>
<li>To copy files from Ubuntu virtualbox: go to settings, add a shared folder, login to ubuntu, go to /media/your_shared_folder (you may need to add user <code>sudo adduser bduser vboxsf</code> and then reboot <code>sudo reboot</code>)</li>
<li>If you are starting a new cookbook, you can use <code>berks cookbook your_cookbook_name</code> to initialize the folder structure (no need to do this for this cookbook - I have done it for you). Refer to the following tutorial is necessary: use external cookbook: <a href="http://docs.aws.amazon.com/opsworks/latest/userguide/cookbooks-101-opsworks-berkshelf.html#cookbooks-101-opsworks-berkshelf-vagrant">http://docs.aws.amazon.com/opsworks/latest/userguide/cookbooks-101-opsworks-berkshelf.html#cookbooks-101-opsworks-berkshelf-vagrant</a>
</li>
<li>You can add external cookbook in Berksfile as <code>cookbook 'java'</code>, you can go to <a href="https://supermarket.chef.io">https://supermarket.chef.io</a> to search for a cookbook and find the related berkshelf information there.</li>
</ul>

<h3>
<a id="references" class="anchor" href="#references" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>References</h3>

<ul>
<li>
<a href="http://www.terpconnect.umd.edu/%7Ekpzhang/">http://www.terpconnect.umd.edu/~kpzhang/</a> (special thanks to my friend Kunpeng for the course materials)</li>
<li><a href="http://www.bogotobogo.com/Hadoop/BigData_hadoop_Install_on_ubuntu_single_node_cluster.php">http://www.bogotobogo.com/Hadoop/BigData_hadoop_Install_on_ubuntu_single_node_cluster.php</a></li>
<li><a href="http://www.michael-noll.com/tutorials/writing-an-hadoop-mapreduce-program-in-python/">http://www.michael-noll.com/tutorials/writing-an-hadoop-mapreduce-program-in-python/</a></li>
<li><a href="https://www.linkedin.com/pulse/getting-started-apache-spark-ubuntu-1404-myles-harrison">https://www.linkedin.com/pulse/getting-started-apache-spark-ubuntu-1404-myles-harrison</a></li>
<li><a href="https://www.digitalocean.com/community/tutorial_series/getting-started-managing-your-infrastructure-using-chef">https://www.digitalocean.com/community/tutorial_series/getting-started-managing-your-infrastructure-using-chef</a></li>
<li><a href="https://www.linkedin.com/pulse/installing-hbase-112-over-hadoop-271in-modeon-ubuntu-1404-sharma">https://www.linkedin.com/pulse/installing-hbase-112-over-hadoop-271in-modeon-ubuntu-1404-sharma</a></li>
</ul>

      <footer class="site-footer">
        <span class="site-footer-owner"><a href="https://github.com/harrywang/bigdata-cookbook">Big Data 101 Cookbook</a> is maintained by <a href="https://github.com/harrywang">harrywang</a>.</span>

        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a> using the <a href="https://github.com/jasonlong/cayman-theme">Cayman theme</a> by <a href="https://twitter.com/jasonlong">Jason Long</a>.</span>
      </footer>

    </section>

   <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-126223-8', 'auto');
  ga('send', 'pageview');
</script>
  </body>
</html>
